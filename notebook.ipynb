{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My project for Optimization Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate linear regression implemented through solving an optimization problem using the gradient descent method and some of its variants.\n",
    "\n",
    "The dataset used was taken from Kaggle.com and concerns house prices in the New York area. The idea is to perform a linear regression using the price as the dependent variable and the number of bedrooms, number of bathrooms, and the size of the house as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"dataset/NY-House-Dataset.csv\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELIMINO LE COLONNE NON DESIDERATE\n",
    "\n",
    "columns_to_drop = [\"ADDRESS\", \"STATE\", \"MAIN_ADDRESS\", \"ADMINISTRATIVE_AREA_LEVEL_2\", \n",
    "                   \"LOCALITY\", \"STREET_NAME\", \"LONG_NAME\", \"FORMATTED_ADDRESS\", \n",
    "                   \"LATITUDE\", \"LONGITUDE\",\"TYPE\",\"SUBLOCALITY\",\"BROKERTITLE\"]\n",
    "df_filtered = df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "#FILTRO GLI OUTLIER\n",
    "\n",
    "vars = [\"PRICE\", \"BEDS\", \"BATH\", \"PROPERTYSQFT\"]\n",
    "#df_filtered = df.copy()\n",
    "\n",
    "# Filtra i valori anomali utilizzando il concetto di IQR\n",
    "for var in vars:\n",
    "    Q1 = df_filtered[var].quantile(0.25)\n",
    "    Q3 = df_filtered[var].quantile(0.75)\n",
    "    \n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_filtered = df_filtered[(df_filtered[var] >= lower_limit) & (df_filtered[var] <= upper_limit)]\n",
    "    \n",
    "print(df_filtered.head)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Model:\n",
    "\n",
    "$y = w_0 + w_1 x1 + w_2 x2 + w_3 x3$\n",
    "\n",
    "In this case:\n",
    "\n",
    "$PRICE = w_0 + w_1 BEDS + w_2 BATH + w_3 PROPERTYSQFT$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data points (mean 0 and std dev 1).\"\"\"\n",
    "    x = x - np.mean(x)\n",
    "    x = x / np.std(x)\n",
    "    return x\n",
    "\n",
    "def build_model_data(x1, x2, x3, y):\n",
    "    \"\"\"Get regression data in matrix form with three predictors.\"\"\"\n",
    "    b = y  \n",
    "    num_samples = len(b)  \n",
    "    A = np.c_[np.ones(num_samples), x1, x2, x3]\n",
    "    \n",
    "    return A, b\n",
    "\n",
    "\n",
    "PRICE=df_filtered.iloc[:,0]\n",
    "BEDS =df_filtered.iloc[:,1]\n",
    "BATH =df_filtered.iloc[:,2]\n",
    "PROPERTYSQFT=df_filtered.iloc[:,3]\n",
    "\n",
    "print (PRICE.head())\n",
    "print(BEDS.head())\n",
    "print(BATH.head())\n",
    "print(PROPERTYSQFT.head())\n",
    "\n",
    "A, b = build_model_data(standardize(BEDS), standardize(BATH),standardize(PROPERTYSQFT),standardize(PRICE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient descendent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objective(Axmb):\n",
    "    \"\"\"Calculate ||Ax - b||^2 for the vector Axmb = Ax - b\"\"\"\n",
    "    # ***************************************************\n",
    "    # YOUR CODE HERE\n",
    "    obj = (Axmb**2).sum()\n",
    "    # ***************************************************    \n",
    "    return obj\n",
    "\n",
    "def compute_gradient(A, x, b):\n",
    "    \"\"\"Compute the gradient and objective function.\"\"\"\n",
    "    # ***************************************************\n",
    "    # YOUR CODE HERE\n",
    "    Axmb = A.dot(x) - b\n",
    "    grad = 2 * A.T.dot(Axmb)\n",
    "    # ***************************************************\n",
    "    return grad, Axmb\n",
    "\n",
    "def gradient_descent(A, initial_x, b, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store x and objective func. values\n",
    "    xs = [initial_x]\n",
    "    objectives = []\n",
    "    x = initial_x\n",
    "    for n_iter in range(max_iters):\n",
    "      \n",
    "        # compute objective and gradient\n",
    "        grad, Axmb = compute_gradient(A, x, b)\n",
    "        obj = calculate_objective(Axmb)\n",
    "\n",
    "        # ***************************************************\n",
    "        # YOUR CODE HERE\n",
    "        # update x by a gradient descent step\n",
    "        x = x - gamma * grad\n",
    "        # ***************************************************\n",
    "        \n",
    "        # store x and objective function value\n",
    "        xs.append(x)\n",
    "        objectives.append(obj)\n",
    "        print(\"Gradient Descent({bi}/{ti}): objective={l:.5f}, x=[{w0:.5f},{w1:.5f}]\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=obj, w0=x[0], w1=x[1]))\n",
    "\n",
    "    return objectives, xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/29): objective=3659.00000, x=[-0.00000,2.26195]\n",
      "Gradient Descent(1/29): objective=136698.31760, x=[-0.00000,-35.16268]\n",
      "Gradient Descent(2/29): objective=23003907.57993, x=[0.00000,467.27816]\n",
      "Gradient Descent(3/29): objective=3966911264.15454, x=[-0.00000,-6141.81526]\n",
      "Gradient Descent(4/29): objective=684263221158.83960, x=[0.00000,80643.94971]\n",
      "Gradient Descent(5/29): objective=118031770228636.46875, x=[-0.00000,-1059011.77904]\n",
      "Gradient Descent(6/29): objective=20359873817378720.00000, x=[0.00000,13908071.09638]\n",
      "Gradient Descent(7/29): objective=3511973956195641344.00000, x=[-0.00000,-182661767.21582]\n",
      "Gradient Descent(8/29): objective=605797526157898678272.00000, x=[0.00000,2399017309.56936]\n",
      "Gradient Descent(9/29): objective=104496971686375373930496.00000, x=[-0.00001,-31507992046.72483]\n",
      "Gradient Descent(10/29): objective=18025192611737807192326144.00000, x=[0.00009,413817250478.64618]\n",
      "Gradient Descent(11/29): objective=3109253440076961720890818560.00000, x=[-0.00078,-5434963340837.66211]\n",
      "Gradient Descent(12/29): objective=536330299646547986378810982400.00000, x=[0.01597,71381340513262.79688]\n",
      "Gradient Descent(13/29): objective=92514230783279147292816150888448.00000, x=[-0.17603,-937503281603579.12500]\n",
      "Gradient Descent(14/29): objective=15958231155432377251428070500335616.00000, x=[1.93597,12312915494292464.00000]\n",
      "Gradient Descent(15/29): objective=2752713171304239842254323182141964288.00000, x=[-22.64003,-161714515055937920.00000]\n",
      "Gradient Descent(16/29): objective=474828928699431846813729847091876855808.00000, x=[296.84797,2123914878118898688.00000]\n",
      "Gradient Descent(17/29): objective=81905559169837349927227577744501518630912.00000, x=[-3504.24003,-27894925880079556608.00000]\n",
      "Gradient Descent(18/29): objective=14128289616427819170787161291988749330153472.00000, x=[42633.10397,366364442363337965568.00000]\n",
      "Gradient Descent(19/29): objective=2437057624767065550621768863988696835539599360.00000, x=[-695564.40003,-4811731897419677302784.00000]\n",
      "Gradient Descent(20/29): objective=420379963016143508106649600144717193421030162432.00000, x=[12457772.94397,63195990592244462649344.00000]\n",
      "Gradient Descent(21/29): objective=72513391357475614928029776699186998572780405915648.00000, x=[-133571115.12003,-829999117176920405442560.00000]\n",
      "Gradient Descent(22/29): objective=12508188754849122745313528078909414620947233284030464.00000, x=[1859293710.22397,10900984826082398857854976.00000]\n",
      "Gradient Descent(23/29): objective=2157598520742813684456551771212260145881389206626893824.00000, x=[-15732892334.19204,-143170598280521477716443136.00000]\n",
      "Gradient Descent(24/29): objective=372174698347661001319730364867026583149149967212053790720.00000, x=[309722549487.50397,1880364071598424861521739776.00000]\n",
      "Gradient Descent(25/29): objective=64198230003645495394425820635215130151391616924350752489472.00000, x=[-4756827031304.30469,-24696195198056861864399208448.00000]\n",
      "Gradient Descent(26/29): objective=11073865993305693999713502320153715261585608038903364303454208.00000, x=[48160468590299.02344,324353175256151470185547563008.00000]\n",
      "Gradient Descent(27/29): objective=1910185187827278031161853408186120989532123941672863040819691520.00000, x=[-834545058374318.25000,-4259967231997979847269055201280.00000]\n",
      "Gradient Descent(28/29): objective=329497165127380702343831117231432705321370726546183287436375752704.00000, x=[9829978859239016.00000,55949262107161653370495973720064.00000]\n",
      "Gradient Descent(29/29): objective=56836574023731432511119787244887339271656084781596868736657087528960.00000, x=[-119297229656727840.00000,-734822537324474658652833403895808.00000]\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 30\n",
    "gamma = 0.001  # gamma = 0.1 does not converge\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_objectives_naive, gradient_xs_naive = gradient_descent(A, x_initial, b, max_iters, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
