{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My project for Optimization Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate linear regression implemented through solving an optimization problem using the gradient descent method and some of its variants.\n",
    "\n",
    "The dataset used was taken from Kaggle.com and concerns house prices in the New York area. The idea is to perform a linear regression using the price as the dependent variable and the number of bedrooms, number of bathrooms, and the size of the house as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         BROKERTITLE                TYPE   \n",
      "0        Brokered by Douglas Elliman  -111 Fifth Ave      Condo for sale  \\\n",
      "1                                Brokered by Serhant      Condo for sale   \n",
      "2                             Brokered by Sowae Corp      House for sale   \n",
      "3                                Brokered by COMPASS      Condo for sale   \n",
      "4  Brokered by Sotheby's International Realty - E...  Townhouse for sale   \n",
      "\n",
      "       PRICE  BEDS       BATH  PROPERTYSQFT   \n",
      "0     315000     2   2.000000        1400.0  \\\n",
      "1  195000000     7  10.000000       17545.0   \n",
      "2     260000     4   2.000000        2015.0   \n",
      "3      69000     3   1.000000         445.0   \n",
      "4   55000000     7   2.373861       14175.0   \n",
      "\n",
      "                                             ADDRESS                    STATE   \n",
      "0                               2 E 55th St Unit 803       New York, NY 10022  \\\n",
      "1  Central Park Tower Penthouse-217 W 57th New Yo...       New York, NY 10019   \n",
      "2                                   620 Sinclair Ave  Staten Island, NY 10312   \n",
      "3                            2 E 55th St Unit 908W33      Manhattan, NY 10022   \n",
      "4                                        5 E 64th St       New York, NY 10065   \n",
      "\n",
      "                                        MAIN_ADDRESS   \n",
      "0             2 E 55th St Unit 803New York, NY 10022  \\\n",
      "1  Central Park Tower Penthouse-217 W 57th New Yo...   \n",
      "2            620 Sinclair AveStaten Island, NY 10312   \n",
      "3         2 E 55th St Unit 908W33Manhattan, NY 10022   \n",
      "4                      5 E 64th StNew York, NY 10065   \n",
      "\n",
      "  ADMINISTRATIVE_AREA_LEVEL_2  LOCALITY      SUBLOCALITY       STREET_NAME   \n",
      "0             New York County  New York        Manhattan  East 55th Street  \\\n",
      "1               United States  New York  New York County          New York   \n",
      "2               United States  New York  Richmond County     Staten Island   \n",
      "3               United States  New York  New York County          New York   \n",
      "4               United States  New York  New York County          New York   \n",
      "\n",
      "          LONG_NAME                                  FORMATTED_ADDRESS   \n",
      "0   Regis Residence  Regis Residence, 2 E 55th St #803, New York, N...  \\\n",
      "1  West 57th Street             217 W 57th St, New York, NY 10019, USA   \n",
      "2   Sinclair Avenue     620 Sinclair Ave, Staten Island, NY 10312, USA   \n",
      "3  East 55th Street               2 E 55th St, New York, NY 10022, USA   \n",
      "4  East 64th Street               5 E 64th St, New York, NY 10065, USA   \n",
      "\n",
      "    LATITUDE  LONGITUDE  \n",
      "0  40.761255 -73.974483  \n",
      "1  40.766393 -73.980991  \n",
      "2  40.541805 -74.196109  \n",
      "3  40.761398 -73.974613  \n",
      "4  40.767224 -73.969856  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"dataset/NY-House-Dataset.csv\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         PRICE  BEDS  BATH  PROPERTYSQFT\n",
      "0      315000     2   2.0   1400.000000\n",
      "2      260000     4   2.0   2015.000000\n",
      "3       69000     3   1.0    445.000000\n",
      "6      899500     2   2.0   2184.207862\n",
      "8      265000     1   1.0    750.000000\n",
      "...       ...   ...   ...           ...\n",
      "4796   599000     1   1.0   2184.207862\n",
      "4797   245000     1   1.0   2184.207862\n",
      "4798  1275000     1   1.0   2184.207862\n",
      "4799   598125     2   1.0    655.000000\n",
      "4800   349000     1   1.0    750.000000\n",
      "\n",
      "[3659 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "#ELIMINO LE COLONNE NON DESIDERATE\n",
    "\n",
    "columns_to_drop = [\"ADDRESS\", \"STATE\", \"MAIN_ADDRESS\", \"ADMINISTRATIVE_AREA_LEVEL_2\", \n",
    "                   \"LOCALITY\", \"STREET_NAME\", \"LONG_NAME\", \"FORMATTED_ADDRESS\", \n",
    "                   \"LATITUDE\", \"LONGITUDE\",\"TYPE\",\"SUBLOCALITY\",\"BROKERTITLE\"]\n",
    "df_filtered = df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "#FILTRO GLI OUTLIER\n",
    "\n",
    "vars = [\"PRICE\", \"BEDS\", \"BATH\", \"PROPERTYSQFT\"]\n",
    "#df_filtered = df.copy()\n",
    "\n",
    "# Filtra i valori anomali utilizzando il concetto di IQR\n",
    "for var in vars:\n",
    "    Q1 = df_filtered[var].quantile(0.25)\n",
    "    Q3 = df_filtered[var].quantile(0.75)\n",
    "    \n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_filtered = df_filtered[(df_filtered[var] >= lower_limit) & (df_filtered[var] <= upper_limit)]\n",
    "    \n",
    "print(df_filtered.head)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Model:\n",
    "\n",
    "$y = w_0 + w_1 x1 + w_2 x2 + w_3 x3$\n",
    "\n",
    "In this case:\n",
    "\n",
    "$PRICE = w_0 + w_1 BEDS + w_2 BATH + w_3 PROPERTYSQFT$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data points (mean 0 and std dev 1).\"\"\"\n",
    "    x = x - np.mean(x)\n",
    "    x = x / np.std(x)\n",
    "    return x\n",
    "\n",
    "def build_model_data(x, y):\n",
    "    \"\"\"Get regression data in matrix form.\"\"\"\n",
    "    b = y\n",
    "    num_samples = len(b)\n",
    "    A = np.c_[np.ones(num_samples), x]\n",
    "    return A, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      BEDS  BATH\n",
      "0        2   2.0\n",
      "2        4   2.0\n",
      "3        3   1.0\n",
      "6        2   2.0\n",
      "8        1   1.0\n",
      "...    ...   ...\n",
      "4796     1   1.0\n",
      "4797     1   1.0\n",
      "4798     1   1.0\n",
      "4799     2   1.0\n",
      "4800     1   1.0\n",
      "\n",
      "[3659 rows x 2 columns]\n",
      "0        315000\n",
      "2        260000\n",
      "3         69000\n",
      "6        899500\n",
      "8        265000\n",
      "         ...   \n",
      "4796     599000\n",
      "4797     245000\n",
      "4798    1275000\n",
      "4799     598125\n",
      "4800     349000\n",
      "Name: PRICE, Length: 3659, dtype: int64\n",
      "Numero di condizionamento di A: 3.034159332975248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = df_filtered[['BEDS','BATH']]\n",
    "prices = df_filtered['PRICE']\n",
    "print(features)\n",
    "print(prices)\n",
    "\n",
    "A, b = build_model_data(standardize(features), standardize(prices))\n",
    "A = np.array(A)\n",
    "b = np.array(b)\n",
    "\n",
    "cond_number = np.linalg.cond(A)\n",
    "print(\"Numero di condizionamento di A:\", cond_number)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples n =  3659\n",
      "Dimension of each sample d =  3\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples n = ', b.shape[0])\n",
    "print('Dimension of each sample d = ', A.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient descendent method functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objective(Axmb):\n",
    "    \"\"\"Calculate ||Ax - b||^2 for the vector Axmb = Ax - b\"\"\"\n",
    "    # ***************************************************\n",
    "    # YOUR CODE HERE\n",
    "    obj = (Axmb**2).sum()\n",
    "    # ***************************************************    \n",
    "    return obj\n",
    "\n",
    "def compute_gradient(A, x, b):\n",
    "    \"\"\"Compute the gradient and objective function.\"\"\"\n",
    "    # ***************************************************\n",
    "    # YOUR CODE HERE\n",
    "    Axmb = A.dot(x) - b\n",
    "    grad = 2 * A.T.dot(Axmb)\n",
    "    # ***************************************************\n",
    "    return grad, Axmb\n",
    "\n",
    "def gradient_descent(A, initial_x, b, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store x and objective func. values\n",
    "    xs = [initial_x]\n",
    "    objectives = []\n",
    "    x = initial_x\n",
    "    for n_iter in range(max_iters):\n",
    "      \n",
    "        # compute objective and gradient\n",
    "        grad, Axmb = compute_gradient(A, x, b)\n",
    "        obj = calculate_objective(Axmb)\n",
    "\n",
    "        # ***************************************************\n",
    "        # YOUR CODE HERE\n",
    "        # update x by a gradient descent step\n",
    "        x = x - gamma * grad\n",
    "        # ***************************************************\n",
    "        \n",
    "        # store x and objective function value\n",
    "        xs.append(x)\n",
    "        objectives.append(obj)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): objective={l:.5f}, x=[{w0:.5f},{w1:.5f}]\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=obj, w0=x[0], w1=x[1]))\n",
    "\n",
    "    return objectives, xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test Classic Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_iters = 500\n",
    "gamma = 0.00001  # Tasso di apprendimento iniziale\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_objectives_naive, gradient_xs_naive = gradient_descent(A, x_initial, b, max_iters, gamma)\n",
    "print(\"########################\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try also to use a smoothness gradient descent with L parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_L(A, b):\n",
    "    \"\"\"Calculate the smoothness constant for f\"\"\"\n",
    "    # ***************************************************\n",
    "    # YOUR CODE HERE\n",
    "    # compute L = smoothness constant of f\n",
    "    L = 2 * np.linalg.norm(A.T.dot(A), ord=2)\n",
    "    # ***************************************************\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_iters = 500\n",
    "gamma_smooth = (1/calculate_L(A,b))\n",
    "print(gamma_smooth)\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_objectives_smooth, gradient_xs_smooth = gradient_descent(A, x_initial, b, max_iters, gamma_smooth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIPSCHITZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 500\n",
    "R = 25\n",
    "\n",
    "# ***************************************************\n",
    "# YOUR CODE HERE\n",
    "# Compute the bound B on the gradient norm\n",
    "B = 2 * (R*np.linalg.norm(np.dot(A.T,A)) + np.linalg.norm(np.dot(A.T,b)))\n",
    "# ***************************************************\n",
    "\n",
    "# ***************************************************\n",
    "# YOUR CODE HERE\n",
    "#  Compute learning rate based on bounded gradient\n",
    "gamma_bounded = R/(B*np.sqrt(max_iters))\n",
    "# ***************************************************\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_objectives_bounded, gradient_xs_bounded = gradient_descent(A, x_initial, b, max_iters, gamma_bounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Objective Function')\n",
    "plt.plot(range(len(gradient_objectives_smooth)), gradient_objectives_smooth,'green', label='gradient descent assuming smoothness')\n",
    "plt.plot(range(len(gradient_objectives_naive)), gradient_objectives_naive,'red', label='naive gradient descent')\n",
    "plt.plot(range(len(gradient_objectives_bounded)), gradient_objectives_bounded,'blue', label='gradient descent assuming bounded gradients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOCHASTIC GRADIENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient(A, x, b):\n",
    "    \"\"\"\n",
    "    Compute a mini-batch stochastic gradient from a subset of `num_examples` from the dataset.\n",
    "    \n",
    "    :param b: a numpy array of shape (num_examples)\n",
    "    :param A: a numpy array of shape (num_examples, num_features)\n",
    "    :param x: compute the mini-batch gradient at these parameters, numpy array of shape (num_features)\n",
    "    \n",
    "    :return: gradient: numpy array of shape (num_features)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # YOUR CODE HERE\n",
    "    # TODO: implement gradient computation.\n",
    "    # ***************************************************\n",
    "    batch_size = len(b)\n",
    "    Axmb = A.dot(x) - b\n",
    "    gradient = A.T.dot(Axmb) / batch_size\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10\n",
    "for i in range(num_trials):\n",
    "    # Try different parameter vectors $x$\n",
    "    x = np.random.rand(A.shape[1])\n",
    "\n",
    "    stochastic_gradients = []\n",
    "    # *********************************************************\n",
    "    # YOUR CODE HERE\n",
    "    # TODO: Compute all stochastic gradients, mini-batch size 1\n",
    "    # *********************************************************\n",
    "    for i in range(len(b)):\n",
    "        stochastic_gradients.append(minibatch_gradient(A[i:i+1, :], x, b[i:i+1]))\n",
    "\n",
    "    # axis = 0 is the mean along the column\n",
    "    mean_stochastic_gradients = np.mean(stochastic_gradients, axis = 0)\n",
    "\n",
    "    # *********************************************************\n",
    "    # YOUR CODE HERE\n",
    "    # *********************************************************\n",
    "    full_gradient = minibatch_gradient(A, x, b)\n",
    "\n",
    "    # Those should be the same on average\n",
    "    assert np.allclose(mean_stochastic_gradients, full_gradient)\n",
    "    \n",
    "print('Tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient(A, x, b, batch_size=1):\n",
    "    \"\"\"Compute a stochastic gradient\"\"\"\n",
    "    dataset_size = len(b)\n",
    "    indices = np.random.choice(dataset_size, batch_size, replace=False)\n",
    "    return minibatch_gradient(A[indices, :], x, b[indices])\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        data_A, \n",
    "        initial_x, \n",
    "        targets_b, \n",
    "        batch_size, \n",
    "        max_iters, \n",
    "        learning_rate):\n",
    "    \"\"\"\n",
    "    Mini-batch Stochastic Gradient Descent for Linear Least Squares problems.\n",
    "    \n",
    "    :param data_A: numpy array of size (num_examples, num_features)\n",
    "    :param initial_x: starting parameters, a numpy array of size (num_features)\n",
    "    :param targets_b: numpy array of size (num_examples)\n",
    "    :param batch_size: size of the mini-batches\n",
    "    :param max_iters: integer, number of updates to do\n",
    "    :param learning_rate: float\n",
    "    :param decreasing_learning_rate: if true, the learning rate should decay as 1 / t \n",
    "    \n",
    "    :return:\n",
    "    - objectives, a list of loss values on the whole dataset, collected at the end of each pass over the dataset (epoch)\n",
    "    - xs, a list of parameter vectors, collected at the end of each pass over the dataset\n",
    "    \"\"\"\n",
    "    xs = [initial_x]  # parameters after each update \n",
    "    objectives = []  # loss values after each update\n",
    "    x = initial_x\n",
    "    \n",
    "    # ***************************************************\n",
    "    # YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    for iteration in range(max_iters):\n",
    "        grad = stochastic_gradient(data_A, x, targets_b, batch_size=batch_size)\n",
    "\n",
    "        # update x through the stochastic gradient update\n",
    "        x = x - learning_rate * grad\n",
    "\n",
    "        # store x and objective\n",
    "        xs.append(x.copy())\n",
    "        objective = full_objective(data_A, x, targets_b)\n",
    "        objectives.append(objective)\n",
    "        \n",
    "        if iteration % 1000 == 0:\n",
    "            print(\"SGD({bi:04d}/{ti:04d}): objective = {l:10.2f}\".format(\n",
    "                  bi=iteration, ti=max_iters - 1, l=objective))\n",
    "\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = int(4000)\n",
    "gamma = 0.001   # this is totally arbitrary\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives, sgd_xs = stochastic_gradient_descent(A, x_initial, b, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "plt.plot(range(len(sgd_objectives)), sgd_objectives,'purple', label='gradient descent assuming smoothness')\n",
    "print(best_objective)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Convergence rates', fontsize = 20)\n",
    "plt.loglog(sgd_objectives - best_objective, label = r'$\\gamma = 0.001$')\n",
    "plt.xlabel('iteration t'  , fontsize = 20)\n",
    "plt.ylabel(r'$f(x_t) - f(x^*)$', fontsize = 20)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METODO Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(A, initial_x, b, max_iters, alpha, beta1, beta2, epsilon):\n",
    "    \"\"\"Adam optimization algorithm.\"\"\"\n",
    "    x = initial_x\n",
    "    m = np.zeros_like(x)  # Moment estimates\n",
    "    v = np.zeros_like(x)  # Squared gradient estimates\n",
    "    t = 0                # Time step\n",
    "    objectives = []\n",
    "    xs = [x]\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        t += 1\n",
    "        grad, Axmb = compute_gradient(A, x, b)\n",
    "        obj = calculate_objective(Axmb)\n",
    "        \n",
    "        # Update moments\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        # Update parameters\n",
    "        x = x - alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        # Store results\n",
    "        xs.append(x)\n",
    "        objectives.append(obj)\n",
    "        \n",
    "        # Print statement for debugging\n",
    "        if n_iter % 100 == 0:\n",
    "            print(f\"Adam({n_iter}/{max_iters - 1}): objective={obj:.5f}, x=[{x[0]:.5f},{x[1]:.5f}]\")\n",
    "    \n",
    "    return objectives, xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Tasso di apprendimento\n",
    "beta1 = 0.9    # Coefficiente per il momento del primo ordine\n",
    "beta2 = 0.999  # Coefficiente per il momento del secondo ordine\n",
    "epsilon = 1e-8 # Termine di regolarizzazione\n",
    "max_iters = 50\n",
    "initial_x = np.zeros(A.shape[1])\n",
    "objectives, xs = adam(A, initial_x, b, max_iters, alpha, beta1, beta2, epsilon)\n",
    "plt.plot(range(len(objectives)), objectives,'purple', label='gradient descent assuming smoothness')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
